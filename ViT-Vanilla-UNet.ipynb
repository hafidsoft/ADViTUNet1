{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpeTmvK5NwIx4v09S6NjRm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Detection and localization of anomalous objects in Video Sequences using Vision Transformer and U-Net Model |\n","Stage 2\n","\n","* ViT & Vanilla U-Net"],"metadata":{"id":"c8Yf1jQXg7Mw"}},{"cell_type":"code","source":["import os #to joint the path\n","from glob import glob #used to extract images and masks path\n","from sklearn.model_selection import train_test_split #to split the dataset to train and validation\n","from tqdm import tqdm #The progress bar\n","import cv2"],"metadata":{"id":"cA9Pe2BOZ2AG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wHLl8xcXg0W","executionInfo":{"status":"ok","timestamp":1713818478058,"user_tz":-60,"elapsed":25726,"user":{"displayName":"ABDELHAFID BERROUKHAM","userId":"08802074107343609957"}},"outputId":"5d2c4aaf-e15b-4a68-d1c1-0a78a2ee6fd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","source":["def create_dir(path):\n","  \"\"\" Create a directory \"\"\"\n","  if not os.path.exists(path):\n","    os.makedirs(path)"],"metadata":{"id":"84gTTXWqZrrV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Prepar train and valid dataset"],"metadata":{"id":"11P1Ch9-wREM"}},{"cell_type":"code","source":["create_dir('Data')\n","create_dir('Data/Dataset')\n","create_dir('Data/TrainValid')"],"metadata":{"id":"Jg8tDh-5iGLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Paths\n","dataset_path = '/content/Data/Dataset/ped1'"],"metadata":{"id":"vUtbCJfOiIj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy Data folder from drive\n","!cp -r '/content/drive/MyDrive/Colab Notebooks/ADViTUNet/Data/Preprocess/' '/content/Data/'\n","!cp -r '/content/drive/MyDrive/Colab Notebooks/ADViTUNet/Dataset/ped1/' '/content/Data/Dataset/'"],"metadata":{"id":"rGO8zl7_iKq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r '/content/Data/Preprocess/FramesComb/' '/content/Data/Dataset/ped1'"],"metadata":{"id":"kE2f8ljPiNkJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.utils.validation import sp\n","def load_data(path,split=0.2):\n","  \"\"\" Load frames and masks \"\"\"\n","  frames = sorted(glob(f\"{path}/FramesComb/*.jpg\"))\n","  masks = sorted(glob(f\"{path}/Frames_GT/*.bmp\"))\n","  # print(\"Number of frames : \",len(frames))\n","  # print(\"Number of masks : \", len(masks))\n","\n","  \"\"\" Split the data \"\"\"\n","  split_size = int(len(frames) * split)\n","  train_x, valid_x = train_test_split(frames, test_size=split_size, random_state=42)\n","  train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n","\n","  return (train_x, train_y), (valid_x, valid_y)"],"metadata":{"id":"XvSkUmrFfIg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Load tha dataset \"\"\"\n","(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path,split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"dD8SowIOf3t6","executionInfo":{"status":"ok","timestamp":1713828619798,"user_tz":-60,"elapsed":17,"user":{"displayName":"ABDELHAFID BERROUKHAM","userId":"08802074107343609957"}},"outputId":"988ef07b-70db-4395-b3fa-64b126cc5afb"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' Load tha dataset '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["create_dir(\"/content/Data/TrainValid/train/frames/\")\n","create_dir(\"/content/Data/TrainValid/train/masks/\")\n","create_dir(\"/content/Data/TrainValid/valid/frames/\")\n","create_dir(\"/content/Data/TrainValid/valid/masks/\")"],"metadata":{"id":"qkT7M1ZfhG6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Paths\n","TrainPath = '/content/Data/TrainValid/train/'\n","ValidPath = '/content/Data/TrainValid/valid/'"],"metadata":{"id":"E_Adcjy2i9Xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from albumentations import HorizontalFlip, VerticalFlip, Rotate #Data augmentation"],"metadata":{"id":"MLZZAx02jjkj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_data(images, masks, save_path, augment=False):\n","  \"\"\" Performing data augmentation \"\"\"\n","  H = 512\n","  W = 512\n","\n","  for idx, (x,y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n","    \"\"\"Extracting the dir name and image name\"\"\"\n","    dir_name=x.split(\"/\")[-3]\n","    name=dir_name + \"_\" + x.split(\"/\")[-1].split(\".\")[0]\n","\n","    \"\"\"Read the image and name\"\"\"\n","    x=cv2.imread(x, cv2.IMREAD_COLOR)\n","    y=cv2.imread(y, cv2.IMREAD_COLOR)\n","\n","    if augment== True:\n","      aug = HorizontalFlip(p=1.0) #p:probability of applying this data augmentation\n","      augmented = aug(image=x, mask=y)\n","      x1=augmented[\"image\"]\n","      y1=augmented[\"mask\"]\n","\n","      aug = VerticalFlip(p=1)\n","      augmented = aug(image=x, mask=y)\n","      x2=augmented[\"image\"]\n","      y2=augmented[\"mask\"]\n","\n","      aug = Rotate(limit=45, p=1.0)\n","      augmented = aug(image=x, mask=y)\n","      x3=augmented[\"image\"]\n","      y3=augmented[\"mask\"]\n","\n","      X=[x, x1, x2, x3]\n","      Y=[y, y1, y2, y3]\n","\n","    else:\n","      X=[x]\n","      Y=[y]\n","\n","    idx = 0\n","    for i, m in zip(X, Y):\n","      i=cv2.resize(i,(W,H))\n","      m=cv2.resize(m, (W,H))\n","      m=m/255.0\n","      m=(m > 0.5)*255\n","\n","      if len(X) == 1:\n","        tmp_image_name = f\"{name}.jpg\"\n","        tmp_mask_name = f\"{name}.jpg\"\n","      else:\n","        tmp_image_name = f\"{name}_{idx}.jpg\"\n","        tmp_mask_name = f\"{name}_{idx}.jpg\"\n","\n","      image_path = os.path.join(save_path,\"frames/\", tmp_image_name)\n","      mask_path = os.path.join(save_path,\"masks/\", tmp_mask_name)\n","\n","      cv2.imwrite(image_path, i)\n","      cv2.imwrite(mask_path, m)\n","\n","      idx += 1\n","\n","    # break\n"],"metadata":{"id":"2HcUCgV0hbKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# augment_data(train_x, train_y,\"new_data/train/\", augment=True)\n","augment_data(train_x, train_y, TrainPath, augment=False)\n","augment_data(valid_x, valid_y, ValidPath, augment=False) #we dont apply data augmentation for validation data"],"metadata":{"id":"yFdoYEA9iHWo","executionInfo":{"status":"ok","timestamp":1713828633992,"user_tz":-60,"elapsed":286,"user":{"displayName":"ABDELHAFID BERROUKHAM","userId":"08802074107343609957"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Vanilla U-Net Model"],"metadata":{"id":"BXR3neDCJXRt"}},{"cell_type":"code","source":["from torch import nn"],"metadata":{"id":"lQ2FZS-ALNGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(DoubleConv, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","\n","            nn.Conv2d(in_ch, out_ch, 3, padding = 1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace = True),\n","\n","            nn.Conv2d(out_ch, out_ch, 3, padding = 1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace = True)\n","\n","        )\n","\n","    def forward(self, input):\n","\n","        return self.conv(input)\n","\n","class InputConvolution(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(InputConvolution, self).__init__()\n","        self.inp_conv = DoubleConv(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        x = self.inp_conv(x)\n","        return x\n","\n","class Up(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(Up, self).__init__()\n","        self.up_conv = nn.ConvTranspose2d(in_ch, out_ch, kernel_size = 2, stride = 2)\n","        self.conv = DoubleConv(in_ch, out_ch)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up_conv(x1)\n","        x = torch.cat([x2, x1], dim = 1)\n","        x = self.conv(x)\n","\n","        return x\n","\n","class Down(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(Down, self).__init__()\n","        self.pool = nn.MaxPool2d(2)\n","        self.conv = DoubleConv(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        x = self.pool(x)\n","        x = self.conv(x)\n","        return x\n","\n","class LastConvolution(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(LastConvolution, self).__init__()\n","        self.conv1 = nn.Conv2d(in_ch, out_ch, 1)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        return x"],"metadata":{"id":"RQoH8LnUXFMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VUnet(nn.Module):\n","    def __init__(self, channels, classes):\n","        super(VUnet, self).__init__()\n","        self.inp = InputConvolution(channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.down4 = Down(512, 1024)\n","        self.up1 = Up(1024, 512)\n","        self.up2 = Up(512, 256)\n","        self.up3 = Up(256, 128)\n","        self.up4 = Up(128, 64)\n","        self.out = LastConvolution(64, classes)\n","\n","    def forward(self, x):\n","        x1 = self.inp(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x6 = self.up1(x5, x4)\n","        x7 = self.up2(x6, x3)\n","        x8 = self.up3(x7, x2)\n","        x9 = self.up4(x8, x1)\n","        x10 = self.out(x9)\n","        return x10"],"metadata":{"id":"RrKYMKKaJsth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"ZQFr6R4nTOIo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vmodel = VUnet(3,1).to(device)\n","\n","from torchsummary import summary\n","summary(vmodel, input_size=(3, 512, 512))"],"metadata":{"id":"oxkSNSEcMELu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713818684592,"user_tz":-60,"elapsed":1949,"user":{"displayName":"ABDELHAFID BERROUKHAM","userId":"08802074107343609957"}},"outputId":"c736bc17-0049-4b62-f751-54b463e28a28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 512, 512]           1,792\n","       BatchNorm2d-2         [-1, 64, 512, 512]             128\n","              ReLU-3         [-1, 64, 512, 512]               0\n","            Conv2d-4         [-1, 64, 512, 512]          36,928\n","       BatchNorm2d-5         [-1, 64, 512, 512]             128\n","              ReLU-6         [-1, 64, 512, 512]               0\n","        DoubleConv-7         [-1, 64, 512, 512]               0\n","  InputConvolution-8         [-1, 64, 512, 512]               0\n","         MaxPool2d-9         [-1, 64, 256, 256]               0\n","           Conv2d-10        [-1, 128, 256, 256]          73,856\n","      BatchNorm2d-11        [-1, 128, 256, 256]             256\n","             ReLU-12        [-1, 128, 256, 256]               0\n","           Conv2d-13        [-1, 128, 256, 256]         147,584\n","      BatchNorm2d-14        [-1, 128, 256, 256]             256\n","             ReLU-15        [-1, 128, 256, 256]               0\n","       DoubleConv-16        [-1, 128, 256, 256]               0\n","             Down-17        [-1, 128, 256, 256]               0\n","        MaxPool2d-18        [-1, 128, 128, 128]               0\n","           Conv2d-19        [-1, 256, 128, 128]         295,168\n","      BatchNorm2d-20        [-1, 256, 128, 128]             512\n","             ReLU-21        [-1, 256, 128, 128]               0\n","           Conv2d-22        [-1, 256, 128, 128]         590,080\n","      BatchNorm2d-23        [-1, 256, 128, 128]             512\n","             ReLU-24        [-1, 256, 128, 128]               0\n","       DoubleConv-25        [-1, 256, 128, 128]               0\n","             Down-26        [-1, 256, 128, 128]               0\n","        MaxPool2d-27          [-1, 256, 64, 64]               0\n","           Conv2d-28          [-1, 512, 64, 64]       1,180,160\n","      BatchNorm2d-29          [-1, 512, 64, 64]           1,024\n","             ReLU-30          [-1, 512, 64, 64]               0\n","           Conv2d-31          [-1, 512, 64, 64]       2,359,808\n","      BatchNorm2d-32          [-1, 512, 64, 64]           1,024\n","             ReLU-33          [-1, 512, 64, 64]               0\n","       DoubleConv-34          [-1, 512, 64, 64]               0\n","             Down-35          [-1, 512, 64, 64]               0\n","        MaxPool2d-36          [-1, 512, 32, 32]               0\n","           Conv2d-37         [-1, 1024, 32, 32]       4,719,616\n","      BatchNorm2d-38         [-1, 1024, 32, 32]           2,048\n","             ReLU-39         [-1, 1024, 32, 32]               0\n","           Conv2d-40         [-1, 1024, 32, 32]       9,438,208\n","      BatchNorm2d-41         [-1, 1024, 32, 32]           2,048\n","             ReLU-42         [-1, 1024, 32, 32]               0\n","       DoubleConv-43         [-1, 1024, 32, 32]               0\n","             Down-44         [-1, 1024, 32, 32]               0\n","  ConvTranspose2d-45          [-1, 512, 64, 64]       2,097,664\n","           Conv2d-46          [-1, 512, 64, 64]       4,719,104\n","      BatchNorm2d-47          [-1, 512, 64, 64]           1,024\n","             ReLU-48          [-1, 512, 64, 64]               0\n","           Conv2d-49          [-1, 512, 64, 64]       2,359,808\n","      BatchNorm2d-50          [-1, 512, 64, 64]           1,024\n","             ReLU-51          [-1, 512, 64, 64]               0\n","       DoubleConv-52          [-1, 512, 64, 64]               0\n","               Up-53          [-1, 512, 64, 64]               0\n","  ConvTranspose2d-54        [-1, 256, 128, 128]         524,544\n","           Conv2d-55        [-1, 256, 128, 128]       1,179,904\n","      BatchNorm2d-56        [-1, 256, 128, 128]             512\n","             ReLU-57        [-1, 256, 128, 128]               0\n","           Conv2d-58        [-1, 256, 128, 128]         590,080\n","      BatchNorm2d-59        [-1, 256, 128, 128]             512\n","             ReLU-60        [-1, 256, 128, 128]               0\n","       DoubleConv-61        [-1, 256, 128, 128]               0\n","               Up-62        [-1, 256, 128, 128]               0\n","  ConvTranspose2d-63        [-1, 128, 256, 256]         131,200\n","           Conv2d-64        [-1, 128, 256, 256]         295,040\n","      BatchNorm2d-65        [-1, 128, 256, 256]             256\n","             ReLU-66        [-1, 128, 256, 256]               0\n","           Conv2d-67        [-1, 128, 256, 256]         147,584\n","      BatchNorm2d-68        [-1, 128, 256, 256]             256\n","             ReLU-69        [-1, 128, 256, 256]               0\n","       DoubleConv-70        [-1, 128, 256, 256]               0\n","               Up-71        [-1, 128, 256, 256]               0\n","  ConvTranspose2d-72         [-1, 64, 512, 512]          32,832\n","           Conv2d-73         [-1, 64, 512, 512]          73,792\n","      BatchNorm2d-74         [-1, 64, 512, 512]             128\n","             ReLU-75         [-1, 64, 512, 512]               0\n","           Conv2d-76         [-1, 64, 512, 512]          36,928\n","      BatchNorm2d-77         [-1, 64, 512, 512]             128\n","             ReLU-78         [-1, 64, 512, 512]               0\n","       DoubleConv-79         [-1, 64, 512, 512]               0\n","               Up-80         [-1, 64, 512, 512]               0\n","           Conv2d-81          [-1, 1, 512, 512]              65\n","  LastConvolution-82          [-1, 1, 512, 512]               0\n","================================================================\n","Total params: 31,043,521\n","Trainable params: 31,043,521\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 3.00\n","Forward/backward pass size (MB): 4208.00\n","Params size (MB): 118.42\n","Estimated Total Size (MB): 4329.42\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["##Trainig dataset\n","from os.path import ismount\n","import os\n","# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","import numpy as np\n","import cv2\n","from glob import glob\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import Recall, Precision\n"],"metadata":{"id":"ZmWfM0vvlK5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for V-Unet\n","train_x = sorted(glob(TrainPath + \"frames/*\"))\n","train_y = sorted(glob(TrainPath + \"masks/*\"))\n","\n","valid_x = sorted(glob(ValidPath + \"frames/*\"))\n","valid_y = sorted(glob(ValidPath + \"masks/*\"))\n","\n","data_str = f\"Dataset Size:\\nTrain x: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n","\n","# print(data_str)"],"metadata":{"id":"EZ1sl8UDUs99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class DriveDataset(Dataset):\n","    def __init__(self, images_path, masks_path):\n","\n","        self.images_path = images_path\n","        self.masks_path = masks_path\n","        self.n_samples = len(images_path)\n","\n","    def __getitem__(self, index):\n","        \"\"\" Reading image \"\"\"\n","        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n","        image = image/255.0 ## (512, 512, 3)\n","        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n","        image = image.astype(np.float32)\n","        image = torch.from_numpy(image)\n","\n","        \"\"\" Reading mask \"\"\"\n","        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n","        mask = mask/255.0   ## (512, 512)\n","        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n","        mask = mask.astype(np.float32)\n","        mask = torch.from_numpy(mask)\n","\n","        return image, mask\n","\n","    def __len__(self):\n","        return self.n_samples"],"metadata":{"id":"oMWg2myAW2Uc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for V-Unet\n","# train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n","# valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n","train_dataset = DriveDataset(train_x, train_y)\n","valid_dataset = DriveDataset(valid_x, valid_y)"],"metadata":{"id":"tILN1qKHTTaF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Metrics"],"metadata":{"id":"jWAxRp5tkjNk"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import  backend as k"],"metadata":{"id":"zHcQl4tektfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def iou(y_true, y_pred):\n","  def f(y_true, y_pred):\n","    intersection = (y_true * y_pred).sum()\n","    union = y_true.sum() + y_pred.sum() - intersection\n","    x = (intersection + 1e-15) / (union + 1e-15)\n","    x = x.astype(np.float32)\n","    return x\n","  return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n","\n","smooth = 1e-15\n","\n","def dice_coef(y_true, y_pred):\n","  y_true = tf.keras.layers.Flatten()(y_true)\n","  y_pred = tf.keras.layers.Flatten()(y_pred)\n","  intersection = tf.reduce_sum(y_true * y_pred)\n","  return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)+smooth)\n","\n","def dice_loss(y_true, y_pred):\n","  return 1.0 - dice_coef(y_true, y_pred)"],"metadata":{"id":"PY_nYuvbkvg-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Training the Vanilla U-Net model"],"metadata":{"id":"i6vGS-CobADA"}},{"cell_type":"code","source":["from torchsummary import summary\n","import time\n","from torch.utils.data import DataLoader"],"metadata":{"id":"N6RcpQKwLnid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Calculate the time taken \"\"\"\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"metadata":{"id":"6HMT_pAeNSBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, loader, optimizer, loss_fn, device):\n","    epoch_loss = 0.0\n","\n","    model.train()\n","    for x, y in loader:\n","        x = x.to(device, dtype=torch.float32)\n","        y = y.to(device, dtype=torch.float32)\n","\n","        optimizer.zero_grad()\n","        y_pred = model(x)\n","        loss = loss_fn(y_pred, y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    epoch_loss = epoch_loss/len(loader)\n","    return epoch_loss"],"metadata":{"id":"WGYKcAbVQFwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, loader, loss_fn, device):\n","    epoch_loss = 0.0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device, dtype=torch.float32)\n","            y = y.to(device, dtype=torch.float32)\n","\n","            y_pred = model(x)\n","            loss = loss_fn(y_pred, y)\n","            epoch_loss += loss.item()\n","\n","        epoch_loss = epoch_loss/len(loader)\n","    return epoch_loss\n"],"metadata":{"id":"8AE-1crmi1vy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","\n","        return 1 - dice\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = torch.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","\n","        return Dice_BCE"],"metadata":{"id":"hkusHe8DRGSu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_dir(\"files\")"],"metadata":{"id":"eIb4ZVkY4TXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\"\"\"Seeding\"\"\"\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"metadata":{"id":"q-5uTlPO5Q0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["H = 512\n","W = 512\n","size = (H, W)\n","batch_size = 2\n","num_epochs = 20\n","lr = 1e-4\n","checkpoint_path = \"files/checkpoint.pth\"\n","\n","# train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,num_workers=2)\n","train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n","valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","device = torch.device('cuda')\n","vmodel = VUnet(3,1)\n","vmodel = vmodel.to(device)\n","\n","# summary(vmodel, input_size=(3, 512, 512))\n","\n","optimizer = torch.optim.Adam(vmodel.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n","loss_fn = DiceBCELoss()\n","\n","best_valid_loss = float(\"inf\")\n","\n","for epoch in range(num_epochs):\n","  start_time = time.time()\n","  train_loss = train(vmodel, train_loader, optimizer, loss_fn, device)\n","  valid_loss = evaluate(vmodel, valid_loader, loss_fn, device)\n","\n","  \"\"\" Saving the model \"\"\"\n","  if valid_loss < best_valid_loss:\n","    data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n","    print(data_str)\n","\n","    best_valid_loss = valid_loss\n","    torch.save(vmodel.state_dict(), checkpoint_path)\n","\n","\n","  end_time = time.time()\n","  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","  data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n","  data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n","  data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n","  print(data_str)"],"metadata":{"id":"7KFo5vaqKXEx","executionInfo":{"status":"ok","timestamp":1713828833905,"user_tz":-60,"elapsed":286,"user":{"displayName":"ABDELHAFID BERROUKHAM","userId":"08802074107343609957"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!cp -r '/content/files' '/content/drive/MyDrive/Colab Notebooks/ADViTUNet/Models/V-Unet/'"],"metadata":{"id":"Gib1f6_VDmh0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Evaluation"],"metadata":{"id":"6ZbAdU4-r5_0"}},{"cell_type":"code","source":["# seeding\n","create_dir(\"Results\")"],"metadata":{"id":"wYDUkmi_r_IE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score,recall_score"],"metadata":{"id":"4cMeisyu65mg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_metrics(y_true, y_pred):\n","    \"\"\" Ground truth \"\"\"\n","    y_true = y_true.cpu().numpy()\n","    y_true = y_true > 0.5\n","    y_true = y_true.astype(np.uint8)\n","    y_true = y_true.reshape(-1)\n","\n","    \"\"\" Prediction \"\"\"\n","    y_pred = y_pred.cpu().numpy()\n","    y_pred = y_pred > 0.5\n","    y_pred = y_pred.astype(np.uint8)\n","    y_pred = y_pred.reshape(-1)\n","\n","    score_jaccard = jaccard_score(y_true, y_pred)\n","    score_f1 = f1_score(y_true, y_pred)\n","    score_recall = recall_score(y_true, y_pred)\n","    score_precision = precision_score(y_true, y_pred)\n","    score_acc = accuracy_score(y_true, y_pred)\n","\n","    return [score_jaccard, score_f1, score_recall, score_precision, score_acc]\n","\n","\n","def mask_parse(mask):\n","    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n","    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n","    return mask"],"metadata":{"id":"in-F81f25U9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from operator import add\n","H = 512\n","W = 512\n","size = (H, W)\n","\n","checkpoint_path = \"files/checkpoint.pth\"\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","vmodel = VUnet(3,1)\n","vmodel = vmodel.to(device)\n","vmodel.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","vmodel.eval()\n","\n","metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]\n","time_taken = []\n","\n","for i, (x, y) in tqdm(enumerate(zip(valid_x, valid_y)), total=len(valid_x)):\n","        \"\"\" Extract the name \"\"\"\n","        name = x.split(\"/\")[-1].split(\".\")[0]\n","\n","        \"\"\" Reading image \"\"\"\n","        image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n","        ## image = cv2.resize(image, size)\n","        x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n","        x = x/255.0\n","        x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n","        x = x.astype(np.float32)\n","        x = torch.from_numpy(x)\n","        x = x.to(device)\n","\n","        \"\"\" Reading mask \"\"\"\n","        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n","        ## mask = cv2.resize(mask, size)\n","        y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)\n","        y = y/255.0\n","        y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)\n","        y = y.astype(np.float32)\n","        y = torch.from_numpy(y)\n","        y = y.to(device)\n","\n","\n","        with torch.no_grad():\n","            \"\"\" Prediction and Calculating FPS \"\"\"\n","            start_time = time.time()\n","            pred_y = vmodel(x)\n","            pred_y = torch.sigmoid(pred_y)\n","            total_time = time.time() - start_time\n","            time_taken.append(total_time)\n","\n","\n","            score = calculate_metrics(y, pred_y)\n","            metrics_score = list(map(add, metrics_score, score))\n","            print(\"-------\",metrics_score)\n","            pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n","            pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n","            pred_y = pred_y > 0.5\n","            pred_y = np.array(pred_y, dtype=np.uint8)\n","\n","        \"\"\" Saving masks \"\"\"\n","        ori_mask = mask_parse(mask)\n","        pred_y = mask_parse(pred_y)\n","        line = np.ones((size[1], 10, 3)) * 128\n","\n","        cat_images = np.concatenate(\n","            [image, line, ori_mask, line, pred_y * 255], axis=1\n","        )\n","        cv2.imwrite(f\"Results/{name}.png\", cat_images)\n","\n","\n","jaccard = metrics_score[0]/len(valid_x)\n","f1 = metrics_score[1]/len(valid_x)\n","recall = metrics_score[2]/len(valid_x)\n","precision = metrics_score[3]/len(valid_x)\n","acc = metrics_score[4]/len(valid_x)\n","print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f}\")\n","\n","fps = 1/np.mean(time_taken)\n","print(\"FPS: \", fps)"],"metadata":{"id":"SyfBoJIAtJ8d","executionInfo":{"status":"ok","timestamp":1713828782479,"user_tz":-60,"elapsed":277,"user":{"displayName":"ABDELHAFID BERROUKHAM","userId":"08802074107343609957"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#Save results to Colab\n","!cp -r '/content/Results' '/content/drive/MyDrive/Colab Notebooks/ADViTUNet/VResults'"],"metadata":{"id":"p6odtw6IGwlp"},"execution_count":null,"outputs":[]}]}